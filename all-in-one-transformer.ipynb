{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.fft\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import torchmetrics\n",
    "import wandb\n",
    "import gc\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from dataset import CoughDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Transformer implementation. Make sure to run the cell. Also, we recommend you do this after getting your SelfAttention implementation working\n",
    "# WRITTEN BY Holger Severin Bovbjerg <hsbo@es.aau.dk>\n",
    "# Adapted by Sarthak Yadav <sarthaky@es.aau.dk> for this assignment\n",
    "# KWT model based on model from https://github.com/ID56/Torch-KWT/blob/main/models/kwt.py\"\"\"\n",
    "\n",
    "\n",
    "# Basically vision transformer, ViT that accepts MFCC + SpecAug. Refer to:\n",
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, fn):\n",
    "        \"\"\"\n",
    "        Initialises PreNorm module\n",
    "        :param dim: model dimension\n",
    "        :param fn: torch module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward method for PreNorm module\n",
    "        :param x: input tensor\n",
    "        :param kwargs: Keyword arguments\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class PostNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Post layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, fn):\n",
    "        \"\"\"\n",
    "        Initialises PostNorm module\n",
    "        :param dim: model dimension\n",
    "        :param fn: torch module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward method for PostNorm module\n",
    "        :param x: input tensor\n",
    "        :param kwargs: Keyword arguments\n",
    "        :return: PostNorm output\n",
    "        \"\"\"\n",
    "        return self.norm(self.fn(x, **kwargs))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed forward model\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        \"\"\"\n",
    "        Initialises FeedForward module\n",
    "        :param dim: feedforward dim\n",
    "        :param hidden_dim: hidden dimension of feedforward layer\n",
    "        :param dropout: feedforward dropout percentage\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward method for feedforward module\n",
    "        :param x: input tensor\n",
    "        :return: FeedForward output\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SHA(nn.Module):\n",
    "    def __init__(self, head_dim, attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        # print(f\"in SHA, q:{q.shape}, k:{k.shape}, v:{v.shape}\")\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v)\n",
    "        # print(\"output shape:\", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim_head\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            SHA(self.head_dim) for _ in range(self.num_heads)\n",
    "        ])\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "                nn.Linear(inner_dim, dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ) if project_out else nn.Identity() \n",
    "  \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward method for Attention module\n",
    "        :param x: input tensor\n",
    "        :return: Attention module output\n",
    "        \"\"\"\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 3, 0, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        o = []\n",
    "        for i in range(self.num_heads):\n",
    "            head_i = self.attention_heads[i](q[i],k[i],v[i]).unsqueeze(0)\n",
    "            o.append(head_i)\n",
    "        o = torch.concat(o, dim=0)\n",
    "        o = o.permute(1, 2, 0, 3).reshape(B, N, -1)\n",
    "\n",
    "        return self.to_out(o)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, pre_norm=True, dropout=0., mha_block=MHA):\n",
    "        \"\"\"\n",
    "        Initialises Transformer model\n",
    "        :param dim: transformer dimension\n",
    "        :param depth: number of transformer layers\n",
    "        :param heads: number of attention heads for each transformer layer\n",
    "        :param dim_head: dimension of each attention head\n",
    "        :param mlp_dim: MLP dimension\n",
    "        :param pre_norm: specifies whether PreNorm (True) or PostNorm (False) is used\n",
    "        :param dropout: dropout percentage of Attention of FeedForward modules\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        P_Norm = PreNorm if pre_norm else PostNorm\n",
    "        if mha_block is None:\n",
    "          mha_block = MHA\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                P_Norm(dim, mha_block(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                P_Norm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward method for Transformer model\n",
    "        :param x: input tensor\n",
    "        :return: Tuple of model output, hidden states of transformer and attentions from each transformer layer\n",
    "        \"\"\"\n",
    "        hidden_states = []\n",
    "        attentions = []\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            attentions.append(x)\n",
    "            x = ff(x) + x\n",
    "            hidden_states.append(x)\n",
    "        return x, hidden_states, attentions\n",
    "\n",
    "\n",
    "class KWT(nn.Module):\n",
    "    \"\"\"\n",
    "    KWT model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_res, patch_res, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=1,\n",
    "                 dim_head=64, dropout=0., emb_dropout=0., pre_norm=True, mha_block=MHA, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialises KWT model\n",
    "        :param input_res: input spectrogram size\n",
    "        :param patch_res: patch size\n",
    "        :param num_classes: number of keyword classes\n",
    "        :param dim: transformer dimension\n",
    "        :param depth: number of transformer layers\n",
    "        :param heads: number of attention heads\n",
    "        :param mlp_dim: MLP dimension\n",
    "        :param pool: specifies whether CLS token or average pooling of transformer model is used for classification\n",
    "        :param channels: Number of input channels\n",
    "        :param dim_head: dimension of attention heads\n",
    "        :param dropout: dropout of transformer attention and feed forward layers\n",
    "        :param emb_dropout: dropout of embeddings\n",
    "        :param pre_norm: specifies whether PreNorm (True) or PostNorm (False) is used\n",
    "        :param kwargs: Keyword arguments\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        num_patches = int(input_res[0] / patch_res[0] * input_res[1] / patch_res[1])\n",
    "\n",
    "        patch_dim = channels * patch_res[0] * patch_res[1]\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_res[0], p2=patch_res[1]),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.mask_embedding = nn.Parameter(torch.FloatTensor(dim).uniform_())\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, pre_norm, dropout, mha_block=mha_block)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        # Create classification head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None, output_hidden_states=False, output_attentions=False):\n",
    "        \"\"\"\n",
    "        Forward method of KWT model\n",
    "        :param x: input tensor\n",
    "        :param mask: input mask\n",
    "        :param output_hidden_states: specifies whether hidden states are output\n",
    "        :param output_attentions: specifies whether attentions are output\n",
    "        :return: KWT model output, if output_hidden_states and/or output_attentions the classification head is skipped\n",
    "        \"\"\"\n",
    "        x = self.to_patch_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Add cls token embedding\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Mask input\n",
    "        if mask is not None:\n",
    "            x[mask] = self.mask_embedding\n",
    "\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, hidden_states, attentions = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "\n",
    "        if any([output_hidden_states, output_attentions]):\n",
    "            outputs = (self.mlp_head(x), hidden_states) if output_hidden_states else (self.mlp_head(x), )\n",
    "            outputs = outputs + (attentions, ) if output_attentions else outputs\n",
    "            return outputs\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoughDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 df, \n",
    "                 data_path, \n",
    "                 batch_size=32, \n",
    "                 num_workers=4, \n",
    "                 train_size=0.8, \n",
    "                 val_size=0.1, \n",
    "                 test_size=0.1,\n",
    "                 duration=10.0,\n",
    "                 sample_rate=48000,\n",
    "                 channels=1,\n",
    "                 spectrogram_type='mel-spectrogram',\n",
    "                 n_mels=64,\n",
    "                 n_fft=1024, \n",
    "                 top_db=80,\n",
    "                 augment_masking_val='min',\n",
    "                 n_freq_masks=2,\n",
    "                 n_time_masks=1,\n",
    "                 max_mask_pct=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        self.duration = duration\n",
    "        self.sample_rate = sample_rate\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.spectrogram_type = spectrogram_type\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.top_db = top_db\n",
    "        \n",
    "        self.augment_masking_val = augment_masking_val\n",
    "        self.n_freq_masks = n_freq_masks\n",
    "        self.n_time_masks = n_time_masks\n",
    "        self.max_mask_pct = max_mask_pct\n",
    "        \n",
    "        #self.train_dataset = 0\n",
    "        #self.val_dataset = 0\n",
    "        #self.test_dataset = 0\n",
    "        \n",
    "        if self.train_size + self.val_size + self.test_size != 1.0:\n",
    "            raise Exception('train_size + val_size + test_size must be equal to 1.0')\n",
    "    \n",
    "        self.dataset1 = CoughDataset(df=self.df, \n",
    "                            data_path=self.data_path,\n",
    "                            duration=self.duration,\n",
    "                            sample_rate=self.sample_rate,\n",
    "                            channels=self.channels,\n",
    "                            spectrogram_type=self.spectrogram_type,\n",
    "                            n_mels=self.n_mels,\n",
    "                            n_fft=self.n_fft,\n",
    "                            top_db=self.top_db,\n",
    "                            n_freq_masks=0,\n",
    "                            n_time_masks=0)\n",
    "        \n",
    "        self.dataset2 = CoughDataset(df=self.df, \n",
    "                            data_path=self.data_path,\n",
    "                            duration=self.duration,\n",
    "                            sample_rate=self.sample_rate,\n",
    "                            channels=self.channels,\n",
    "                            spectrogram_type=self.spectrogram_type,\n",
    "                            n_mels=self.n_mels,\n",
    "                            n_fft=self.n_fft,\n",
    "                            top_db=self.top_db,\n",
    "                            augment_masking_val=self.augment_masking_val,\n",
    "                            n_freq_masks=self.n_freq_masks,\n",
    "                            n_time_masks=self.n_time_masks,\n",
    "                            max_mask_pct=self.max_mask_pct)\n",
    "        \n",
    "        self.dataset = torch.utils.data.ConcatDataset([self.dataset1, self.dataset2])\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(self.dataset, [self.train_size, self.val_size, self.test_size])\n",
    "        #self.train_dataset1, self.val_dataset1, self.test_dataset1 = random_split(self.dataset1, [self.train_size, self.val_size, self.test_size])\n",
    "        #self.train_dataset2, self.val_dataset2, self.test_dataset2 = random_split(self.dataset2, [self.train_size, self.val_size, self.test_size])\n",
    "            \n",
    "        #self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset1, self.train_dataset2])\n",
    "        #self.val_dataset = torch.utils.data.ConcatDataset([self.val_dataset1, self.val_dataset2])\n",
    "        #self.test_dataset = torch.utils.data.ConcatDataset([self.test_dataset1, self.test_dataset2])\n",
    "    def train_dataloader(self):\n",
    "        # Concat the two datasets\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_FILE = 'data/metadata_compiled.csv'\n",
    "DATA_PATH = 'data/'\n",
    "\n",
    "\n",
    "metadata_df = pd.read_csv(METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan_df = metadata_df[metadata_df['status'].isna() == False]\n",
    "filtered_df = not_nan_df[not_nan_df['cough_detected'] > 0.9]    # TODO: Set as a hyperparameter\n",
    "filtered_df[['uuid', 'cough_detected', 'SNR', 'age', 'gender', 'status']]\n",
    "# Augment the Covid-19 samples\n",
    "\n",
    "# get size of Covid-19 samples\n",
    "covid_size = filtered_df[filtered_df['status'] == 'COVID-19'].shape[0]\n",
    "filtered_df = filtered_df[filtered_df['status'] == 'healthy'].sample(n=covid_size, random_state=42).append(filtered_df[filtered_df['status'] != 'healthy'])\n",
    "filtered_df = filtered_df[filtered_df['status'] == 'symptomatic'].sample(n=covid_size, random_state=42).append(filtered_df[filtered_df['status'] != 'symptomatic'])\n",
    "print(filtered_df['status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title self-attention\n",
    "from torch.nn.modules.module import T\n",
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, head_dim):\n",
    "    super().__init__()\n",
    "    self.head_dim = head_dim\n",
    "    ## Complete what the scale should be. Then uncomment\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "\n",
    "  def forward(self, q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    q, k, v are query, key and value tensors, respectively of shapes (B, N, C)\n",
    "    where B is Batch Size\n",
    "          N is number of patches\n",
    "          C is the dimensions\n",
    "\n",
    "    You have to \n",
    "\n",
    "    1. compute the dot product between q and k\n",
    "    2. scale it\n",
    "    3. apply softmax to get weights\n",
    "    4. project v with weights\n",
    "    \"\"\"\n",
    "    \"\"\"qk_dot = torch.matmul(q[0,:,:],k[0,:,:].T)\n",
    "    wv = qk_dot * self.scale\n",
    "    n = torch.nn.Softmax(dim=1)\n",
    "    wv_softmax = n(wv)\n",
    "    wv_softmax_weighted = torch.matmul(wv_softmax, v)\"\"\"\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MultiHeadAttention, already implemented\n",
    "class MultiHeadAttentionCustom(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim_head\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            SelfAttention(self.head_dim) for _ in range(self.num_heads)\n",
    "        ])\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "                nn.Linear(inner_dim, dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ) if project_out else nn.Identity() \n",
    "  \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward method for Attention module\n",
    "        :param x: input tensor\n",
    "        :return: Attention module output\n",
    "        \"\"\"\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        #print(\"in attention qkv shape:\", qkv.shape)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 3, 0, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        o = []\n",
    "        for i in range(self.num_heads):\n",
    "            head_i = self.attention_heads[i](q[i],k[i],v[i]).unsqueeze(0)\n",
    "            o.append(head_i)\n",
    "        o = torch.concat(o, dim=0)\n",
    "        o = o.permute(1, 2, 0, 3).reshape(B, N, -1)\n",
    "\n",
    "        return self.to_out(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(trained_model, set):\n",
    "    predictor = trained_model.model.eval().to(torch.device('cuda'))\n",
    "    test_set = set\n",
    "\n",
    "    labels, predictions = [], []\n",
    "    for i in range(len(test_set)):\n",
    "        img, label = test_set[i]\n",
    "        labels.append(label)\n",
    "        with torch.no_grad():\n",
    "            img = img.to(torch.device('cuda'))\n",
    "            pred = predictor(torch.unsqueeze(img, 0))[0].cpu().numpy()\n",
    "            predictions.append(np.argmax(pred))\n",
    "        \n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    plt.figure()\n",
    "    cm = confusion_matrix(labels, predictions, labels=[0,1,2])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['COVID-19', 'healthy', 'symptomatic'])\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title This is the PyTorch-Lightning Module implementation, which takes care of a lot of boilerplate code related to training for us.\n",
    "class KWTLightning(pl.LightningModule):\n",
    "  def __init__(self, hparams, mha_block):\n",
    "    super().__init__()\n",
    "    #self.w1 = hparams[\"w1\"]\n",
    "    #self.w2 = hparams[\"w2\"]\n",
    "    #self.w3 = hparams[\"w3\"]\n",
    "    self.model = KWT(**hparams, mha_block=mha_block)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    return torch.optim.AdamW(self.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "  \n",
    "  def training_step(self, train_batch, batch_idx):\n",
    "    x, y = train_batch\n",
    "    # make a weight tensor with size of 3\n",
    "    #w = torch.ones(3)\n",
    "    # set the weight of the 0 class to 2\n",
    "    #w[0] = self.w1 # Covid\n",
    "    #w[1] = self.w2 # Healthy\n",
    "    #w[2] = self.w3 #\n",
    "    #w = w.to(device='cuda:0')\n",
    "    preds = self.model(x)\n",
    "    loss = nn.functional.cross_entropy(preds, y)\n",
    "    acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "    self.log(\"train_loss\", loss)\n",
    "    self.log(\"train_accuracy\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "    return loss\n",
    "  \n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "    x, y = val_batch\n",
    "    preds = self.model(x)\n",
    "    loss = nn.functional.cross_entropy(preds, y)\n",
    "    acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "    self.log('val_loss', loss)\n",
    "    self.log(\"val_accuracy\", acc, prog_bar=True, on_epoch=True)\n",
    "  \n",
    "  def test_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    preds = self.model(x)\n",
    "    loss = nn.functional.cross_entropy(preds, y)\n",
    "    acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "    self.log('test_loss', loss)\n",
    "    self.log(\"test_accuracy\", acc)\n",
    "    \n",
    "    \n",
    "def create_pl(mha_block, MODEL_HPARAMS):\n",
    "  kwt_pl = KWTLightning(MODEL_HPARAMS, mha_block)\n",
    "  return kwt_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        try:    \n",
    "            torch.manual_seed(69) # noice\n",
    "            \n",
    "            wandb_logger = WandbLogger(log_model=True)\n",
    "            \n",
    "            print(\"Loading data...\")\n",
    "            # Load first data patch, with no augmentation\n",
    "            data_module = CoughDataModule(df=filtered_df, \n",
    "                              data_path=DATA_PATH,\n",
    "                              duration=10.0,\n",
    "                              sample_rate=config.sample_rate,\n",
    "                              channels=1,\n",
    "                              n_mels=config.n_mels,\n",
    "                              n_fft=config.n_fft,\n",
    "                              top_db=80,\n",
    "                              )\n",
    "            # Load second data patch, with augmentation\n",
    "            data_module_aug = CoughDataModule(df=filtered_df, \n",
    "                              data_path=DATA_PATH,\n",
    "                              duration=10.0,\n",
    "                              sample_rate=config.sample_rate,\n",
    "                              channels=1,\n",
    "                              n_mels=config.n_mels,\n",
    "                              n_fft=config.n_fft,\n",
    "                              top_db=80,\n",
    "                              augment=True,\n",
    "            )\n",
    "            # Get size of random sample of data\n",
    "            size = data_module.train_dataset[0][0].shape\n",
    "        \n",
    "            BATCH_SIZE = 128\n",
    "            N_CLASSES = 3\n",
    "            heads = 8\n",
    "            dim = 512\n",
    "            MODEL_HPARAMS = {\n",
    "                \"input_res\":[size[1], size[2]],\n",
    "                \"patch_res\":[size[1], 1],\n",
    "                \"num_classes\":N_CLASSES,\n",
    "                \"mlp_dim\":256,\n",
    "                \"dim\":dim,\n",
    "                \"heads\":heads,\n",
    "                \"dim_head\":dim//heads,       # dim_head should be dim divided by number of heads\n",
    "                \"depth\":6,\n",
    "                \"dropout\": 0.1,\n",
    "                \"emb_dropout\": 0.1\n",
    "            }\n",
    "            print(\"Size of random sample of data:\", size)\n",
    "            kwt_pl = create_pl(mha_block=MultiHeadAttentionCustom, MODEL_HPARAMS=MODEL_HPARAMS)\n",
    "            cuda_available = torch.cuda.is_available()\n",
    "            # Combine the two datamodules into one\n",
    "            print(\"Combining datasets...\")\n",
    "            train_set = torch.utils.data.ConcatDataset([data_module.train_dataset, data_module_aug.train_dataset])\n",
    "            val_set = torch.utils.data.ConcatDataset([data_module.val_dataset, data_module_aug.val_dataset])\n",
    "            test_set = torch.utils.data.ConcatDataset([data_module.test_dataset, data_module_aug.test_dataset])\n",
    "            print(data_module.dataset.label_encoder.classes_)\n",
    "            # Print how many Covid, Normal, and Pneumonia samples are in each set\n",
    "            #print(\"Test set class counts:\", count_labels(data_module.test_dataset))\n",
    "            #print(\"Train set class counts:\", count_labels(data_module.train_dataset))\n",
    "            #print(\"Val set class counts:\", count_labels(data_module.val_dataset))\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=config.max_epochs,\n",
    "                logger=wandb_logger,\n",
    "                accelerator='gpu' if cuda_available else None,\n",
    "            )\n",
    "            trainer.fit(kwt_pl, datamodule=data_module)\n",
    "            #trainer.fit(kwt_pl, data_module)\n",
    "            trainer.test(kwt_pl, test_set)\n",
    "            # plot confusion matrix from trainer\n",
    "            plot_confusion_matrix(kwt_pl, data_module.test_dataset)\n",
    "            #plot_confusion_matrix(kwt_pl, data_module.val_dataset)\n",
    "            #plot_confusion_matrix(kwt_pl, data_module.train_dataset)\n",
    "            \n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            wandb.finish()\n",
    "            raise e\n",
    "        \n",
    "        del wandb_logger\n",
    "        del data_module\n",
    "        del model\n",
    "        del classifier\n",
    "        del trainer\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        # set api key\n",
    "        # api_keys = [\"72b34cc4c01608a271beb1a67ff4ce2ed54642b5\",\n",
    "        #             \"2b1482b5655f56c6d538026fc7aa21fbc5d48ae2\",\n",
    "        #             \"b37bf4266eea06e99d3d420a06937fa903d20e81\",\n",
    "        #             \"db8b5ecab7932ebabc0b4abb2805c3cd32ab76d1\"]\n",
    "        \n",
    "        # # choose random api key\n",
    "        # key = random.choice(api_keys)\n",
    "        # os.environ['WANDB_API_KEY'] = key\n",
    "        config = wandb.config\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        try:    \n",
    "            torch.manual_seed(69) # noice\n",
    "            \n",
    "            wandb_logger = WandbLogger(log_model=True)\n",
    "            \n",
    "            print(\"Loading data...\")\n",
    "            # Load first data patch, with no augmentation\n",
    "            data_module = CoughDataModule(df=filtered_df, \n",
    "                              data_path=DATA_PATH,\n",
    "                              sample_rate=config.sample_rate,\n",
    "                              batch_size=config.batch_size,\n",
    "                              spectrogram_type=config.spectrogram_type,\n",
    "                              n_mels=config.n_mels,\n",
    "                              n_fft=config.n_fft,\n",
    "                              augment_masking_val=config.augment_masking_val,\n",
    "                              n_freq_masks=config.n_freq_masks,\n",
    "                              n_time_masks=config.n_time_masks)\n",
    "            # Load second data patch, with augmentation\n",
    "            size = data_module.train_dataset[0][0].shape\n",
    "            \n",
    "            N_CLASSES = 3\n",
    "            heads = config.heads\n",
    "            dim = config.dim\n",
    "            MODEL_HPARAMS = {\n",
    "                \"input_res\":[size[1], size[2]],\n",
    "                \"patch_res\":[size[1], 1],\n",
    "                \"num_classes\":N_CLASSES,\n",
    "                \"mlp_dim\":256,\n",
    "                \"dim\":dim,\n",
    "                \"heads\":heads,\n",
    "                \"dim_head\":dim//heads,       # dim_head should be dim divided by number of heads\n",
    "                \"depth\":6,\n",
    "                \"dropout\": 0.1,\n",
    "                \"emb_dropout\": 0.1\n",
    "            }\n",
    "            print(\"Size of random sample of data:\", size)\n",
    "            kwt_pl = create_pl(mha_block=MultiHeadAttentionCustom, MODEL_HPARAMS=MODEL_HPARAMS)\n",
    "            cuda_available = torch.cuda.is_available()\n",
    "            # Combine the two datamodules into one\n",
    "            \n",
    "            #print(data_module.dataset.label_encoder.classes_)\n",
    "            #print(data_module.dataset.label_encoder.classes_)\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=config.max_epochs,\n",
    "                logger=wandb_logger,\n",
    "                accelerator='gpu' if cuda_available else None,\n",
    "            )\n",
    "            trainer.fit(kwt_pl, data_module)\n",
    "            trainer.test(kwt_pl, data_module)\n",
    "            plot_confusion_matrix(kwt_pl, data_module.test_dataset)\n",
    "            wandb.finish()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            wandb.finish()\n",
    "            raise e\n",
    "        \n",
    "        del wandb_logger\n",
    "        del data_module\n",
    "        del kwt_pl\n",
    "        del trainer\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module = CoughDataModule(df=filtered_df, \n",
    "#                     data_path=DATA_PATH,\n",
    "#                     duration=10.0,\n",
    "#                     sample_rate=16000,\n",
    "#                     channels=1,\n",
    "#                     n_mels=64,\n",
    "#                     n_fft=1024,\n",
    "#                     top_db=80,\n",
    "#                     )\n",
    "\n",
    "# size = data_module.train_dataset[0][0].shape\n",
    "        \n",
    "# BATCH_SIZE = 128\n",
    "# N_CLASSES = 3\n",
    "# heads = 16\n",
    "# dim = 128\n",
    "# MODEL_HPARAMS = {\n",
    "#     \"input_res\":[size[1], size[2]],\n",
    "#     \"patch_res\":[size[1], 1],\n",
    "#     \"num_classes\":N_CLASSES,\n",
    "#     \"mlp_dim\":256,\n",
    "#     \"dim\":dim,\n",
    "#     \"heads\":heads,\n",
    "#     \"dim_head\":dim//heads,       # dim_head should be dim divided by number of heads\n",
    "#     \"depth\":6,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"emb_dropout\": 0.1,\n",
    "#     \"w1\": 1.19070340331838,\n",
    "#     \"w2\": 0.6683332489272846,\n",
    "#     \"w3\": 1.2838483214224414,\n",
    "# }\n",
    "# kwt_pl = create_pl(mha_block=MultiHeadAttentionCustom, MODEL_HPARAMS=MODEL_HPARAMS)\n",
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=5,\n",
    "#     accelerator='gpu',\n",
    "# )\n",
    "# trainer.fit(kwt_pl, datamodule=data_module)\n",
    "\n",
    "# plot_confusion_matrix(kwt_pl, data_module.test_dataset)\n",
    "# #plot_confusion_matrix(kwt_pl, data_module.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"method\": \"bayes\",\n",
    "  \"metric\": {\n",
    "        \"name\": \"val_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"batch_size\": {\n",
    "        \"values\": [8,16]\n",
    "    },\n",
    "    \"max_epochs\": {\n",
    "        \"values\": [5, 10, 20, 30]\n",
    "    },\n",
    "    \"sample_rate\": {\n",
    "      \"values\": [16000] # Increasing this value will have a significant impact how much data you can fit into memory 48000 aprox double the size of 22050\n",
    "    },\n",
    "    \"spectrogram_type\": {\n",
    "      \"values\": ['mel-spectrogram', 'power-spectrogram']\n",
    "    },\n",
    "    \"n_fft\": {\n",
    "      \"values\": [512, 1024]\n",
    "    },\n",
    "    \"n_mels\": {\n",
    "      \"values\": [32, 64]\n",
    "    },\n",
    "    \"augment_masking_val\": {\n",
    "      \"values\": ['min', 'mean']\n",
    "    },\n",
    "    \"n_freq_masks\": {\n",
    "      \"values\": [1, 2]\n",
    "    },\n",
    "    \"n_time_masks\": {\n",
    "      \"values\": [1]\n",
    "    },\n",
    "    \"dim\": {\n",
    "      \"values\": [128, 256]\n",
    "    },\n",
    "    \"heads\": {\n",
    "      \"values\": [4, 8, 16]\n",
    "    },\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='cough-classifier', entity='dl-miniproject')\n",
    "\n",
    "wandb.agent(sweep_id, function=train_new, count=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0b69965270e405d09dae21e4ae2ea2cc233b709569664f2de57e4e4d7e55573"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
