{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchmetrics\n",
    "import wandb\n",
    "import gc\n",
    "import math\n",
    "\n",
    "from dataset import CoughDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Transformer implementation. Make sure to run the cell. Also, we recommend you do this after getting your SelfAttention implementation working\n",
    "# WRITTEN BY Holger Severin Bovbjerg <hsbo@es.aau.dk>\n",
    "# Adapted by Sarthak Yadav <sarthaky@es.aau.dk> for this assignment\n",
    "# KWT model based on model from https://github.com/ID56/Torch-KWT/blob/main/models/kwt.py\"\"\"\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import torch\n",
    "import torch.fft\n",
    "from torch import nn, einsum\n",
    "\n",
    "# Basically vision transformer, ViT that accepts MFCC + SpecAug. Refer to:\n",
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, fn):\n",
    "        \"\"\"\n",
    "        Initialises PreNorm module\n",
    "        :param dim: model dimension\n",
    "        :param fn: torch module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward method for PreNorm module\n",
    "        :param x: input tensor\n",
    "        :param kwargs: Keyword arguments\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class PostNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Post layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, fn):\n",
    "        \"\"\"\n",
    "        Initialises PostNorm module\n",
    "        :param dim: model dimension\n",
    "        :param fn: torch module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward method for PostNorm module\n",
    "        :param x: input tensor\n",
    "        :param kwargs: Keyword arguments\n",
    "        :return: PostNorm output\n",
    "        \"\"\"\n",
    "        return self.norm(self.fn(x, **kwargs))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed forward model\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        \"\"\"\n",
    "        Initialises FeedForward module\n",
    "        :param dim: feedforward dim\n",
    "        :param hidden_dim: hidden dimension of feedforward layer\n",
    "        :param dropout: feedforward dropout percentage\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward method for feedforward module\n",
    "        :param x: input tensor\n",
    "        :return: FeedForward output\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SHA(nn.Module):\n",
    "    def __init__(self, head_dim, attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        # print(f\"in SHA, q:{q.shape}, k:{k.shape}, v:{v.shape}\")\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v)\n",
    "        # print(\"output shape:\", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim_head\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            SHA(self.head_dim) for _ in range(self.num_heads)\n",
    "        ])\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "                nn.Linear(inner_dim, dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ) if project_out else nn.Identity() \n",
    "  \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward method for Attention module\n",
    "        :param x: input tensor\n",
    "        :return: Attention module output\n",
    "        \"\"\"\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 3, 0, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        o = []\n",
    "        for i in range(self.num_heads):\n",
    "            head_i = self.attention_heads[i](q[i],k[i],v[i]).unsqueeze(0)\n",
    "            o.append(head_i)\n",
    "        o = torch.concat(o, dim=0)\n",
    "        o = o.permute(1, 2, 0, 3).reshape(B, N, -1)\n",
    "\n",
    "        return self.to_out(o)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, pre_norm=True, dropout=0., mha_block=MHA):\n",
    "        \"\"\"\n",
    "        Initialises Transformer model\n",
    "        :param dim: transformer dimension\n",
    "        :param depth: number of transformer layers\n",
    "        :param heads: number of attention heads for each transformer layer\n",
    "        :param dim_head: dimension of each attention head\n",
    "        :param mlp_dim: MLP dimension\n",
    "        :param pre_norm: specifies whether PreNorm (True) or PostNorm (False) is used\n",
    "        :param dropout: dropout percentage of Attention of FeedForward modules\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        P_Norm = PreNorm if pre_norm else PostNorm\n",
    "        if mha_block is None:\n",
    "          mha_block = MHA\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                P_Norm(dim, mha_block(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                P_Norm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward method for Transformer model\n",
    "        :param x: input tensor\n",
    "        :return: Tuple of model output, hidden states of transformer and attentions from each transformer layer\n",
    "        \"\"\"\n",
    "        hidden_states = []\n",
    "        attentions = []\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            attentions.append(x)\n",
    "            x = ff(x) + x\n",
    "            hidden_states.append(x)\n",
    "        return x, hidden_states, attentions\n",
    "\n",
    "\n",
    "class KWT(nn.Module):\n",
    "    \"\"\"\n",
    "    KWT model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_res, patch_res, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=1,\n",
    "                 dim_head=64, dropout=0., emb_dropout=0., pre_norm=True, mha_block=MHA, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialises KWT model\n",
    "        :param input_res: input spectrogram size\n",
    "        :param patch_res: patch size\n",
    "        :param num_classes: number of keyword classes\n",
    "        :param dim: transformer dimension\n",
    "        :param depth: number of transformer layers\n",
    "        :param heads: number of attention heads\n",
    "        :param mlp_dim: MLP dimension\n",
    "        :param pool: specifies whether CLS token or average pooling of transformer model is used for classification\n",
    "        :param channels: Number of input channels\n",
    "        :param dim_head: dimension of attention heads\n",
    "        :param dropout: dropout of transformer attention and feed forward layers\n",
    "        :param emb_dropout: dropout of embeddings\n",
    "        :param pre_norm: specifies whether PreNorm (True) or PostNorm (False) is used\n",
    "        :param kwargs: Keyword arguments\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        num_patches = int(input_res[0] / patch_res[0] * input_res[1] / patch_res[1])\n",
    "\n",
    "        patch_dim = channels * patch_res[0] * patch_res[1]\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_res[0], p2=patch_res[1]),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.mask_embedding = nn.Parameter(torch.FloatTensor(dim).uniform_())\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, pre_norm, dropout, mha_block=mha_block)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        # Create classification head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None, output_hidden_states=False, output_attentions=False):\n",
    "        \"\"\"\n",
    "        Forward method of KWT model\n",
    "        :param x: input tensor\n",
    "        :param mask: input mask\n",
    "        :param output_hidden_states: specifies whether hidden states are output\n",
    "        :param output_attentions: specifies whether attentions are output\n",
    "        :return: KWT model output, if output_hidden_states and/or output_attentions the classification head is skipped\n",
    "        \"\"\"\n",
    "        x = self.to_patch_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Add cls token embedding\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Mask input\n",
    "        if mask is not None:\n",
    "            x[mask] = self.mask_embedding\n",
    "\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, hidden_states, attentions = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "\n",
    "        if any([output_hidden_states, output_attentions]):\n",
    "            outputs = (self.mlp_head(x), hidden_states) if output_hidden_states else (self.mlp_head(x), )\n",
    "            outputs = outputs + (attentions, ) if output_attentions else outputs\n",
    "            return outputs\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoughDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 df, \n",
    "                 data_path, \n",
    "                 batch_size=32, \n",
    "                 num_workers=4, \n",
    "                 train_size=0.8, \n",
    "                 val_size=0.1, \n",
    "                 test_size=0.1,\n",
    "                 duration=10.0,\n",
    "                 sample_rate=48000,\n",
    "                 channels=1,\n",
    "                 n_mels=64,\n",
    "                 n_fft=1024, \n",
    "                 top_db=80):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        self.duration = duration\n",
    "        self.sample_rate = sample_rate\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.top_db = top_db\n",
    "        \n",
    "        if self.train_size + self.val_size + self.test_size != 1.0:\n",
    "            raise Exception('train_size + val_size + test_size must be equal to 1.0')\n",
    "    \n",
    "        dataset = CoughDataset(df=self.df, \n",
    "                            data_path=self.data_path,\n",
    "                            duration=self.duration,\n",
    "                            sample_rate=self.sample_rate,\n",
    "                            channels=self.channels,\n",
    "                            n_mels=self.n_mels,\n",
    "                            n_fft=self.n_fft,\n",
    "                            top_db=self.top_db)\n",
    "\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(dataset, [self.train_size, self.val_size, self.test_size])\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_FILE = 'data/metadata_compiled.csv'\n",
    "DATA_PATH = 'data/'\n",
    "\n",
    "\n",
    "metadata_df = pd.read_csv(METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan_df = metadata_df[metadata_df['status'].isna() == False]\n",
    "filtered_df = not_nan_df[not_nan_df['cough_detected'] > 0.9]    # TODO: Set as a hyperparameter\n",
    "filtered_df[['uuid', 'cough_detected', 'SNR', 'age', 'gender', 'status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title self-attention\n",
    "from torch.nn.modules.module import T\n",
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, head_dim):\n",
    "    super().__init__()\n",
    "    self.head_dim = head_dim\n",
    "    ## Complete what the scale should be. Then uncomment\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "\n",
    "  def forward(self, q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    q, k, v are query, key and value tensors, respectively of shapes (B, N, C)\n",
    "    where B is Batch Size\n",
    "          N is number of patches\n",
    "          C is the dimensions\n",
    "\n",
    "    You have to \n",
    "\n",
    "    1. compute the dot product between q and k\n",
    "    2. scale it\n",
    "    3. apply softmax to get weights\n",
    "    4. project v with weights\n",
    "    \"\"\"\n",
    "    \"\"\"qk_dot = torch.matmul(q[0,:,:],k[0,:,:].T)\n",
    "    wv = qk_dot * self.scale\n",
    "    n = torch.nn.Softmax(dim=1)\n",
    "    wv_softmax = n(wv)\n",
    "    wv_softmax_weighted = torch.matmul(wv_softmax, v)\"\"\"\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MultiHeadAttention, already implemented\n",
    "class MultiHeadAttentionCustom(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim_head\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            SelfAttention(self.head_dim) for _ in range(self.num_heads)\n",
    "        ])\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "                nn.Linear(inner_dim, dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ) if project_out else nn.Identity() \n",
    "  \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward method for Attention module\n",
    "        :param x: input tensor\n",
    "        :return: Attention module output\n",
    "        \"\"\"\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        #print(\"in attention qkv shape:\", qkv.shape)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 3, 0, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        o = []\n",
    "        for i in range(self.num_heads):\n",
    "            head_i = self.attention_heads[i](q[i],k[i],v[i]).unsqueeze(0)\n",
    "            o.append(head_i)\n",
    "        o = torch.concat(o, dim=0)\n",
    "        o = o.permute(1, 2, 0, 3).reshape(B, N, -1)\n",
    "\n",
    "        return self.to_out(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title This is the PyTorch-Lightning Module implementation, which takes care of a lot of boilerplate code related to training for us.\n",
    "class KWTLightning(pl.LightningModule):\n",
    "  def __init__(self, hparams, mha_block):\n",
    "    super().__init__()\n",
    "    self.model = KWT(**hparams, mha_block=mha_block)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    return torch.optim.AdamW(self.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "  \n",
    "  def training_step(self, train_batch, batch_idx):\n",
    "    x, y = train_batch\n",
    "    preds = self.model(x)\n",
    "    loss = nn.functional.cross_entropy(preds, y)\n",
    "    acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "\n",
    "    self.log(\"train_loss\", loss)\n",
    "    self.log(\"train_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "    return loss\n",
    "  \n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "    x, y = val_batch\n",
    "    preds = self.model(x)\n",
    "    loss = nn.functional.cross_entropy(preds, y)\n",
    "    acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "    self.log('val_loss', loss)\n",
    "    self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n",
    "  \n",
    "  def test_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    preds = self.model(x)\n",
    "    loss = nn.functional.cross_entropy(preds, y)\n",
    "    acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "    self.log('test_loss', loss)\n",
    "    self.log(\"test_acc\", acc)\n",
    "\n",
    "def create_pl(mha_block, MODEL_HPARAMS):\n",
    "  kwt_pl = KWTLightning(MODEL_HPARAMS, mha_block)\n",
    "  return kwt_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        try:    \n",
    "            torch.manual_seed(69) # noice\n",
    "            \n",
    "            wandb_logger = WandbLogger(log_model=True)\n",
    "            \n",
    "            data_module = CoughDataModule(df=filtered_df, \n",
    "                              data_path=DATA_PATH, \n",
    "                              batch_size=config.batch_size, \n",
    "                              sample_rate=config.sample_rate,\n",
    "                              n_fft=config.n_fft,\n",
    "                              n_mels=config.n_mels)\n",
    "            # Get size of random sample of data\n",
    "            size = data_module.train_dataset[0][0].shape\n",
    "        \n",
    "            BATCH_SIZE = 128\n",
    "            N_CLASSES = 3\n",
    "            MODEL_HPARAMS = {\n",
    "                \"input_res\":[size[1], size[2]],\n",
    "                \"patch_res\":[size[1], 1],\n",
    "                \"num_classes\":N_CLASSES,\n",
    "                \"mlp_dim\":256,\n",
    "                \"dim\":64,\n",
    "                \"heads\":4,\n",
    "                \"dim_head\":64//4,       # dim_head should be dim divided by number of heads\n",
    "                \"depth\":4,\n",
    "                \"dropout\": 0.1,\n",
    "                \"emb_dropout\": 0.1\n",
    "            }\n",
    "            print(\"Size of random sample of data:\", size)\n",
    "            kwt_pl = create_pl(mha_block=MultiHeadAttentionCustom, MODEL_HPARAMS=MODEL_HPARAMS)\n",
    "            cuda_available = torch.cuda.is_available()\n",
    "            train_set = data_module.train_dataloader()\n",
    "            val_set = data_module.val_dataloader()\n",
    "            test_set = data_module.test_dataloader()\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=config.max_epochs,\n",
    "                logger=wandb_logger,\n",
    "                accelerator='gpu' if cuda_available else None,\n",
    "            )\n",
    "            trainer.fit(kwt_pl, train_dataloaders=train_set, val_dataloaders=val_set)\n",
    "            #trainer.fit(kwt_pl, data_module)\n",
    "            trainer.test(kwt_pl, test_set)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            wandb.finish()\n",
    "            raise e\n",
    "        \n",
    "        del wandb_logger\n",
    "        del data_module\n",
    "        del model\n",
    "        del classifier\n",
    "        del trainer\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"method\": \"bayes\",\n",
    "  \"metric\": {\n",
    "        \"name\": \"val_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"batch_size\": {\n",
    "        \"values\": [32, 64, 128]\n",
    "    },\n",
    "    \"max_epochs\": {\n",
    "        \"values\": [2, 4, 6]\n",
    "    },\n",
    "    \"learning_rate\": {\n",
    "        \"min\": 0.001,\n",
    "        \"max\": 0.01\n",
    "    },\n",
    "    \"sample_rate\": {\n",
    "      \"values\": [16000, 22050, 44100, 48000]\n",
    "    },\n",
    "    \"n_fft\": {\n",
    "      \"values\": [512, 1024, 2048]\n",
    "    },\n",
    "    \"n_mels\": {\n",
    "      \"values\": [32, 64]\n",
    "    },\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='cough-classifier', entity='dl-miniproject')\n",
    "\n",
    "wandb.agent(sweep_id, function=train, count=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
